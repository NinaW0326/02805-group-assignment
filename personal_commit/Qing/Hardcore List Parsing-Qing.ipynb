{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardcore List Parsing\n",
    "\n",
    "Ok. you've made it here. We salute you. Below are some tips and trick for you to get started on the non-trivial task of extracting the names of all Marvel and DC characters on Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marvel Characters\n",
    "\n",
    "We will start with the [List of Marvel characters](http://en.wikipedia.org/wiki/List_of_Marvel_Comics_characters). As you can see the list is organized alphabetically. Therefore, you're going to need to do a few things:\n",
    "\n",
    "\n",
    "1. Use Python to generate a list of the API address all of the relevant pages:  [0-9](https://en.wikipedia.org/wiki/List_of_Marvel_Comics_characters:_0%E2%80%939), [A](https://en.wikipedia.org/wiki/List_of_Marvel_Comics_characters:_A), [B](https://en.wikipedia.org/wiki/List_of_Marvel_Comics_characters:_B), [C](https://en.wikipedia.org/wiki/List_of_Marvel_Comics_characters:_C) and all the way up to [Z](https://en.wikipedia.org/wiki/List_of_Marvel_Comics_characters:_Z).\n",
    "\n",
    "\n",
    "2. Let's take a look at the [A-page](https://en.wikipedia.org/wiki/List_of_Marvel_Comics_characters:_A). Your task is to extract the information for each character and save it as a **txt** file. See an example [here](https://github.com/SocialComplexityLab/socialgraphs2020/blob/master/files/week4_example_1.png?raw=true) (where red is a character name and blue is the content associated to the character). \n",
    "   *  You can also see some of the issues you will have to face - [some characters have *redirections* to the *main articles*](https://github.com/SocialComplexityLab/socialgraphs2020/blob/master/files/week4_example_2.png?raw=true). In that case you should follow a link to the main article and save it's content. Keep in mind that links to **main articles** might be defined as `{{main|link_name}}`, `{{Main article|link_name}}`, `{{Main|link_name}}` and more. Thus, when using regular expressions you should find a way to account for that.\n",
    "   * Some characters have multiple *main articles*. At the same time, not all *main articles* are relevant. If you take a look at the character named **Aginar**, you can see that the *main article* redirects users to [List of Eternals](https://en.wikipedia.org/wiki/List_of_Eternals) - it is not specific to **Aginar**.\n",
    "   * Some characters have `#redirects [[link_to_character_page]]` instead of *main articles*\n",
    "   * There are more *edge* cases and you will have to make a lot of decisions along the way (most of the time, there won't be a *correct* way to handle a case)\n",
    "   * Of course you won't have time to tackle all of the issues (and we do not expect you to do so) - this is to say that we do not expect you to build a perfect dataset.\n",
    "\n",
    "\n",
    "3. A couple of extra tips:\n",
    "    * This is an important point: **Don't get the `html` version of the page**, get the standard [wiki markup](https://en.wikipedia.org/wiki/Help:Wiki_markup) which is what you see when you press \"edit\" on a wikipedia page.\n",
    "    * Some pages contain unicode characters, so we recommend you save the files using the [`io.open`](http://stackoverflow.com/questions/5250744/difference-between-open-and-codecs-open-in-python) method with `utf-8` encoding. You can also take a look at `urllib.parse.quote()` - this might be helpfull when encoding the links\n",
    "    * Store the content of all pages. It's up to you how to do this. One strategy is to use Python's built in `pickle` format. Or you can simply write the content of wiki-pages to text files and store those in a folder on your computer. I'm sure there are other ways. It's crucial that you store them in a way that's easy to access, since we'll use these pages a lot throughout the remainder of the course (so you don't want to retrieve them from wikipedia every time).\n",
    "\n",
    "    * **Small hint**: names in the Marvel List are 2nd-level healines (i.e. wiki-markup specifies them as `==Character Name==`). You can use regular expressions to match the pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DC Characters\n",
    "\n",
    "These are even harder than the Marcel characters. It's OK to give up now. (But you've come this far, so why give up now ?)\n",
    "\n",
    "You start at [DC Universe](http://en.wikipedia.org/wiki/List_of_DC_Comics_characters)\n",
    "\n",
    "1. Follow the same strategy as above, loop over the alphabet to retrive links to every DC character with their own wiki page. \n",
    "\n",
    "\n",
    "2. This time the task is a little bit more tricky since you'll be parsing a table to get the page names, see [B](https://en.wikipedia.org/wiki/List_of_DC_Comics_characters:_B): you need to find a way to extract names in the first column (as well as associated links). For this task you can use **BeautifulSoap** (see [here](https://stackoverflow.com/a/53920093)).\n",
    "\n",
    "\n",
    "3. You will have to alter your code, as some pages (see [C](https://en.wikipedia.org/wiki/List_of_DC_Comics_characters:_C)) include lists of character names.\n",
    "    * And pages such [A](https://en.wikipedia.org/wiki/List_of_DC_Comics_characters:_A) contain both list of character names, as well as the content that is similar to Marvel Pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big thanks to TA Germans for parsing these messy and horrible wikipedia lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import re\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Michael_Jackson'\n",
    "response = urllib.request.urlopen(url)\n",
    "data = response.read()      # a `bytes` object\n",
    "text = data.decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0%E2%80%939', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n"
     ]
    }
   ],
   "source": [
    "listOfHeaders= [\"0%E2%80%939\"]\n",
    "for i in range(65,91):\n",
    "    listOfHeaders.append(chr(i))\n",
    "print(listOfHeaders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_list= []\n",
    "\n",
    "baseurl = \"https://en.wikipedia.org/w/api.php?\"\n",
    "action = \"action=query\"\n",
    "content = \"prop=revisions&rvprop=content\"\n",
    "dataformat =\"format=json\"\n",
    "base_title = \"titles=List_of_Marvel_Comics_characters:_\"\n",
    "\n",
    "for t in listOfHeaders:\n",
    "    query = \"{}{}&{}&{}&{}\".format(baseurl, action, content, \"titles=List_of_Marvel_Comics_characters:_\" + t , dataformat)\n",
    "    api_list.append(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/w/api.php?action=query&prop=revisions&rvprop=content&titles=List_of_Marvel_Comics_characters:_0%E2%80%939&format=json'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPageContents(link):\n",
    "    wikiresponse = urllib.request.urlopen(link)\n",
    "    wikidata = wikiresponse.read()\n",
    "    wikitext = wikidata.decode('utf-8')\n",
    "    response = json.loads(wikitext)\n",
    "\n",
    "    pages = response[\"query\"][\"pages\"].keys()\n",
    "    pageContentList = []\n",
    "\n",
    "    for page in pages:\n",
    "        pageContentList.append(response[\"query\"][\"pages\"][page][\"revisions\"][0][\"*\"])\n",
    "    \n",
    "    return pageContentList[0]\n",
    "\n",
    "def cleanText(text):\n",
    "    # remove references\n",
    "    editedText = re.sub(\"<ref*[^~]*?</ref>\", \"\", text)\n",
    "    editedText = re.sub(\"<!--[\\D|\\d]*}}\\n\", \"\", editedText)\n",
    "    return re.sub(\"[\\D|\\d]*?[}}]\\n(?=.)\", \"\", editedText)\n",
    "\n",
    "def match_link(character_text):\n",
    "    return  re.search(\"{{main[\\w|\\s|(|)]*}}\", character_text, re.IGNORECASE)\n",
    "def visit_link(link):\n",
    "    link = re.findall(\"{{main[\\w|\\s|(|)]*}}\", editedText, re.IGNORECASE)\n",
    "    try:\n",
    "        return true\n",
    "    except IndexError:\n",
    "        print(\"not link\")\n",
    "        return false\n",
    "\n",
    "def generate_sub_link(wiki_link):\n",
    "    pre_transformed_link = re.search(\"(?<=[|])[\\w|\\s|()]*\", wiki_link)\n",
    "    if pre_transformed_link:\n",
    "        title_for_link = pre_transformed_link.group().replace(\" \", \"_\")\n",
    "        return \"{}{}&{}&{}&{}\".format(baseurl, action, \"prop=revisions&rvprop=content\", \"titles=\" +  title_for_link , dataformat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SINGLE\n",
    "content = getPageContents(api_list[1])\n",
    "    \n",
    "matchedPatterns= re.findall(\"(?<=\\n)==[\\w|\\s|-]*[^=]==\", content)\n",
    "titles = [w.replace('==', '') for w in matchedPatterns]\n",
    "titles.pop()\n",
    "for index,title in enumerate(titles):\n",
    "\n",
    "    f = open(\"./marvel/\"+ title + \".txt\", \"w\", encoding= \"utf-8\")\n",
    "\n",
    "    if(index != len(titles) -1):\n",
    "        regex= \"(?<=\" + titles[index] + \"==)[\\D|^\\n|\\w]+(?=\\n==\"+titles[index+1] + \")\"\n",
    "    else:\n",
    "        regex= \"(?<=\" + titles[index] + \"==)[\\D|^\\n|\\w]+(?=\\n==)\"\n",
    "   \n",
    "\n",
    "    matchedText= re.findall(regex, content)\n",
    "    try:\n",
    "        single_character_text= cleanText(matchedText[0])\n",
    "\n",
    "        match = match_link(single_character_text) \n",
    "        if match:\n",
    "            link = generate_sub_link(match.group())\n",
    "            link_content = getPageContents(link)\n",
    "            f.write(link_content)\n",
    "        else:\n",
    "            f.write(single_character_text + \"\\n\")\n",
    "\n",
    "    except :\n",
    "        print(\"failed on= \" + title)\n",
    "        pass\n",
    "    f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL HEADERS\n",
    "counter = 0\n",
    "for header in api_list:\n",
    "\n",
    "    content = getPageContents(header)\n",
    "    \n",
    "    matchedPatterns= re.findall(\"(?<=\\n)==[\\w|\\s|-]*[^=]==\", content)\n",
    "    titles = [w.replace('==', '') for w in matchedPatterns]\n",
    "    titles.pop()\n",
    "\n",
    "    for index,title in enumerate(titles):\n",
    "        t= title.strip()\n",
    "        f = open(\"./marvel/\"+ t + \".txt\", \"w\", encoding= \"utf-8\")\n",
    "\n",
    "        if(index != len(titles) -1):\n",
    "            regex= \"(?<=\" + titles[index] + \"==)[\\D|^\\n|\\w]+(?=\\n==\"+titles[index+1] + \")\"\n",
    "        else:\n",
    "            regex= \"(?<=\" + titles[index] + \"==)[\\D|^\\n|\\w]+(?=\\n==)\"\n",
    "    \n",
    "    \n",
    "        matchedText= re.findall(regex, content)\n",
    "        \n",
    "        try:\n",
    "            single_character_text= cleanText(matchedText[0])\n",
    "\n",
    "            match = match_link(single_character_text) \n",
    "            if match:\n",
    "                link = generate_sub_link(match.group())\n",
    "                link_content = getPageContents(link)\n",
    "                f.write(link_content)\n",
    "            else:\n",
    "                f.write(single_character_text + \"\\n\")\n",
    "\n",
    "        except :\n",
    "            print(\"!!!!! \\n\\n failed on= \" + title + \"\\n\\n------------\")\n",
    "            pass\n",
    "        f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
